import os
import requests
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import openai

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®é¡¹ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
TARGET_CATEGORIES = ["All ,cs.AI ,cs.CE ,cs.CL ,cs.CV ,cs.GT ,cs.IT"]  # ç›®æ ‡è®ºæ–‡åˆ†ç±»
WEEK_DAYS = 7  # çˆ¬å–è¿‘7å¤©çš„è®ºæ–‡
LANGUAGE = os.getenv("LANGUAGE", "Chinese or English")  # å‘¨æŠ¥ç”Ÿæˆè¯­è¨€

# åˆå§‹åŒ– OpenAI/DeepSeek å®¢æˆ·ç«¯
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.base_url = os.getenv("OPENAI_BASE_URL", "https://api.silicon.com")
MODEL_NAME = os.getenv("MODEL_NAME", "Weekly arXiv AI Enhanced")

def get_daily_papers(date_str: str) -> list:
    """
    çˆ¬å–æŒ‡å®šæ—¥æœŸçš„æ¯æ—¥è®ºæ–‡é¡µé¢æ•°æ®ï¼ˆé€‚é…åŸé¡¹ç›®çœŸå®HTMLç»“æ„ï¼‰
    :param date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ YYYY-MM-DD
    :return: è®ºæ–‡åˆ—è¡¨
    """
    try:
        # åŸé¡¹ç›®æ¯æ—¥è®ºæ–‡é¡µé¢çš„URLæ ¼å¼
        daily_url = f"https://bku12345.github.io/daily-arXiv-ai-enhanced/{date_str}.html"
        response = requests.get(daily_url, timeout=15)
        
        # é¡µé¢æ— æ³•è®¿é—®åˆ™è¿”å›ç©ºåˆ—è¡¨
        if response.status_code != 200:
            print(f"âš ï¸  {date_str} é¡µé¢æ— æ³•è®¿é—®ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, "html.parser")
        papers = []
        
        # åŸé¡¹ç›®çœŸå®çš„è®ºæ–‡å¡ç‰‡ç±»åï¼šcol-md-6 col-lg-4 mb-4
        paper_items = soup.find_all("div", class_="col-md-6 col-lg-4 mb-4")
        for item in paper_items:
            # è§£ææ ‡é¢˜å’Œé“¾æ¥ï¼ˆåŸé¡¹ç›®æ ‡é¢˜åœ¨h5.card-titleï¼‰
            title_elem = item.find("h5", class_="card-title")
            title = title_elem.text.strip() if title_elem else ""
            url = title_elem.find("a")["href"] if (title_elem and title_elem.find("a")) else ""
            
            # è§£ææ‘˜è¦ï¼ˆåŸé¡¹ç›®æ‘˜è¦åœ¨div.card-textï¼‰
            abstract_elem = item.find("div", class_="card-text")
            abstract = abstract_elem.text.strip() if abstract_elem else ""
            
            # è§£æä½œè€…/åˆ†ç±»ï¼ˆåŸé¡¹ç›®åœ¨smallæ ‡ç­¾ï¼‰
            meta_elem = item.find("small")
            meta_text = meta_elem.text.strip() if meta_elem else ""
            
            # è¿‡æ»¤ç›®æ ‡åˆ†ç±»çš„è®ºæ–‡
            if any(cat.strip() in meta_text for cat in TARGET_CATEGORIES):
                papers.append({
                    "date": date_str,
                    "title": title,
                    "abstract": abstract,
                    "meta": meta_text,  # ä½œè€…+åˆ†ç±»ä¿¡æ¯
                    "url": url
                })
        
        print(f"âœ… {date_str} çˆ¬å–åˆ° {len(papers)} ç¯‡ç›®æ ‡è®ºæ–‡")
        return papers
    except Exception as e:
        print(f"âŒ çˆ¬å–{date_str}å¤±è´¥ï¼š{str(e)}")
        return []

def get_weekly_papers() -> tuple[list, dict]:
    """
    çˆ¬å–è¿‘7å¤©çš„æ‰€æœ‰ç›®æ ‡è®ºæ–‡ï¼Œå¹¶æŒ‰åˆ†ç±»æ•´ç†
    :return: æ‰€æœ‰è®ºæ–‡åˆ—è¡¨ã€æŒ‰åˆ†ç±»åˆ†ç»„çš„è®ºæ–‡å­—å…¸
    """
    weekly_papers = []
    categorized_papers = {cat: [] for cat in TARGET_CATEGORIES}
    
    # ç”Ÿæˆè¿‘7å¤©çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆYYYY-MM-DDï¼‰
    for i in range(WEEK_DAYS):
        target_date = datetime.now() - timedelta(days=i)
        date_str = target_date.strftime("%Y-%m-%d")
        daily_papers = get_daily_papers(date_str)
        weekly_papers.extend(daily_papers)
        
        # æŒ‰åˆ†ç±»åˆ†ç»„
        for paper in daily_papers:
            for cat in TARGET_CATEGORIES:
                if cat in paper["meta"]:
                    categorized_papers[cat].append(paper)
                    break
    
    return weekly_papers, categorized_papers

def generate_weekly_report(categorized_papers: dict) -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå‘¨æŠ¥ï¼ˆé€‚é…DeepSeek/OpenAIï¼‰
    :param categorized_papers: æŒ‰åˆ†ç±»åˆ†ç»„çš„è®ºæ–‡å­—å…¸
    :return: ç”Ÿæˆçš„å‘¨æŠ¥æ–‡æœ¬
    """
    # æ„é€ æç¤ºè¯
    prompt = f"""
    è¯·ä½ ä½œä¸ºAIé¢†åŸŸç ”ç©¶å‘˜ï¼Œç”¨{LANGUAGE}ç”ŸæˆarXivæ¯å‘¨è®ºæ–‡å‘¨æŠ¥ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š
    1. æ•´ä½“æ€»ç»“ï¼šæœ¬å‘¨AI/æœºå™¨å­¦ä¹ é¢†åŸŸçš„æ ¸å¿ƒç ”ç©¶è¶‹åŠ¿ï¼ˆ150å­—å·¦å³ï¼‰ï¼›
    2. åˆ†ç±»è¯¦æƒ…ï¼šæŒ‰{list(categorized_papers.keys())}åˆ†åˆ«æ€»ç»“ï¼Œæ¯ç±»çªå‡º3-5ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼›
    3. å€¼å¾—å…³æ³¨çš„è®ºæ–‡ï¼šä»æ‰€æœ‰è®ºæ–‡ä¸­é€‰3-5ç¯‡ï¼Œåˆ—å‡ºæ ‡é¢˜+æ ¸å¿ƒè´¡çŒ®ï¼ˆ50å­—/ç¯‡ï¼‰ï¼›
    4. è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œç¬¦åˆå­¦æœ¯å‘¨æŠ¥é£æ ¼ï¼Œä¸è¦å†—ä½™å†…å®¹ã€‚
    
    è®ºæ–‡æ•°æ®ï¼š
    {categorized_papers}
    """
    
    try:
        # è°ƒç”¨DeepSeek/OpenAI API
        response = openai.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„AIé¢†åŸŸç ”ç©¶å‘˜ï¼Œæ“…é•¿æ€»ç»“arXivè®ºæ–‡å‘¨æŠ¥"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # é™ä½éšæœºæ€§ï¼Œä¿è¯æ€»ç»“å‡†ç¡®
            max_tokens=2000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ å¤§æ¨¡å‹ç”Ÿæˆå‘¨æŠ¥å¤±è´¥ï¼š{str(e)}")
        # ç”Ÿæˆå…œåº•å‘¨æŠ¥
        category_details = "\n".join([f"### {cat}\n- æœ¬å‘¨å…±{len(papers)}ç¯‡ç›¸å…³è®ºæ–‡" for cat, papers in categorized_papers.items()])
        return f"""# arXiv æ¯å‘¨è®ºæ–‡æ±‡æ€» ({datetime.now().strftime('%Y-%m-%d')})

## æ•´ä½“æ€»ç»“
æœ¬å‘¨æœªæˆåŠŸç”ŸæˆAIé¢†åŸŸç ”ç©¶è¶‹åŠ¿æ€»ç»“ï¼ˆåŸå› ï¼š{str(e)}ï¼‰ã€‚

## åˆ†ç±»è¯¦æƒ…
{category_details}

## å€¼å¾—å…³æ³¨çš„è®ºæ–‡
æš‚æ— ï¼ˆç”Ÿæˆå¤±è´¥ï¼‰
"""

def save_files(weekly_papers: list, report: str):
    """
    ä¿å­˜è®ºæ–‡æ•°æ®åˆ°JSONã€å‘¨æŠ¥åˆ°MDï¼ˆä¿®å¤pandaså‚æ•°é”™è¯¯ï¼‰
    :param weekly_papers: æ‰€æœ‰è®ºæ–‡åˆ—è¡¨
    :param report: ç”Ÿæˆçš„å‘¨æŠ¥æ–‡æœ¬
    """
    try:
        # ä¿å­˜è®ºæ–‡æ•°æ®åˆ°JSONï¼ˆä¿®å¤ï¼šensure_ascii â†’ force_asciiï¼‰
        df = pd.DataFrame(weekly_papers)
        df.to_json(
            "weekly_papers.json",
            orient="records",
            force_ascii=False,  # å…³é”®ä¿®å¤ï¼šæ”¯æŒä¸­æ–‡
            indent=2  # æ ¼å¼åŒ–è¾“å‡ºï¼Œæ–¹ä¾¿æŸ¥çœ‹
        )
        
        # ä¿å­˜å‘¨æŠ¥åˆ°MD
        with open("weekly_report.md", "w", encoding="utf-8") as f:
            f.write(report)
        
        print(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸï¼šweekly_papers.jsonï¼ˆ{len(weekly_papers)}æ¡æ•°æ®ï¼‰ã€weekly_report.md")
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")

if __name__ == "__main__":
    """ä¸»æ‰§è¡Œé€»è¾‘"""
    print("===== å¼€å§‹ç”ŸæˆarXivæ¯å‘¨è®ºæ–‡å‘¨æŠ¥ =====")
    
    # 1. çˆ¬å–æ¯å‘¨è®ºæ–‡
    weekly_papers, categorized_papers = get_weekly_papers()
    total_papers = len(weekly_papers)
    print(f"\nğŸ“Š æœ¬å‘¨å…±çˆ¬å–åˆ° {total_papers} ç¯‡ç›®æ ‡è®ºæ–‡")
    
    # 2. ç”Ÿæˆå‘¨æŠ¥ï¼ˆç©ºæ•°æ®å…œåº•ï¼‰
    if total_papers == 0:
        print("âš ï¸  æœªçˆ¬å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œç”Ÿæˆç©ºå‘¨æŠ¥")
        report = f"""# arXiv æ¯å‘¨è®ºæ–‡æ±‡æ€» ({datetime.now().strftime('%Y-%m-%d')})

## æ•´ä½“æ€»ç»“
æœ¬å‘¨æœªçˆ¬å–åˆ° cs.AI/cs.LG/stat.ML åˆ†ç±»çš„ç›¸å…³è®ºæ–‡ï¼Œè¯·æ£€æŸ¥ï¼š
1. åŸé¡¹ç›®æ¯æ—¥è®ºæ–‡é¡µé¢æ˜¯å¦æ­£å¸¸è®¿é—®ï¼›
2. ç›®æ ‡åˆ†ç±»æ˜¯å¦æ­£ç¡®ï¼›
3. ç½‘ç»œæ˜¯å¦èƒ½è®¿é—®arXivç›¸å…³é¡µé¢ã€‚

## åˆ†ç±»è¯¦æƒ…
- cs.AIï¼š0ç¯‡
- cs.LGï¼š0ç¯‡
- stat.MLï¼š0ç¯‡

## å€¼å¾—å…³æ³¨çš„è®ºæ–‡
æš‚æ— 
"""
    else:
        print("ğŸ“ å¼€å§‹ç”Ÿæˆå‘¨æŠ¥...")
        report = generate_weekly_report(categorized_papers)
    
    # 3. ä¿å­˜æ–‡ä»¶
    save_files(weekly_papers, report)
    print("\n===== å‘¨æŠ¥ç”Ÿæˆæµç¨‹ç»“æŸ =====")
